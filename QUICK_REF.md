# Quick Reference Guide - ETL Pipeline

## ğŸš€ One-Minute Start

```bash
# 1. Install packages (one time)
pip install -r requirement.txt

# 2. Put file in inputs/ folder

# 3. Run pipeline
python main.py

# 4. Choose option 1, press Enter

# 5. Open outputs/cleaned_output.csv in Excel
```

## ğŸ“ Folder Structure

```
ETL-Pipeline/
â”œâ”€â”€ inputs/                    â† YOUR FILES GO HERE
â”‚   â””â”€â”€ your_file.txt
â”œâ”€â”€ outputs/                   â† RESULTS GO HERE
â”‚   â”œâ”€â”€ cleaned_output.csv     â† OPEN IN EXCEL
â”‚   â”œâ”€â”€ dynamic_schema.json
â”‚   â””â”€â”€ processing_metadata.json
â”œâ”€â”€ main.py                    â† RUN THIS
â””â”€â”€ etl_pipeline.py           â† CORE LOGIC
```

## ğŸ® Command Reference

```bash
# Interactive menu
python main.py

# Watch for new files (auto-process)
python main.py watch

# Process existing files
python main.py process

# Process with database
python main.py db
```

## ğŸ“Š What Happens

```
Mixed File (HTML+JSON+Text)
         â†“
    [EXTRACT]
    â”œâ”€â”€ HTML â†’ title, links, text
    â”œâ”€â”€ JSON â†’ all fields, flattened
    â””â”€â”€ Text â†’ paragraphs
         â†“
    [INFER SCHEMA]
    â†’ Find all fields + types
         â†“
    [NORMALIZE]
    â†’ Same columns for all records
         â†“
    [LOAD]
    â”œâ”€â”€ cleaned_output.csv
    â”œâ”€â”€ dynamic_schema.json
    â””â”€â”€ processing_metadata.json
```

## ğŸ“ Input Examples

### HTML
```html
<html>
  <title>My Title</title>
  <body><p>Content here</p></body>
</html>
```

### JSON
```json
{"name": "John", "age": 30}
{"name": "Jane", "age": 25}
```

### Plain Text
```
This is a paragraph.
Another paragraph here.
```

### Mixed (All in one file)
```
<html>...</html>
{"data": "value"}
Some text here
```

âœ… **All work! Mix and match!**

## ğŸ“Š Output Explanation

### cleaned_output.csv
A spreadsheet with all data in rows/columns

```csv
type,title,content,name,age
html,My Title,Content here,None,None
json,None,None,John,30
json,None,None,Jane,25
text,None,This is a paragraph,None,None
```

### dynamic_schema.json
What fields exist and their types

```json
{
  "name": {
    "type": ["str"],
    "nullable": true,
    "present_in": 2
  },
  "age": {
    "type": ["int"],
    "nullable": true,
    "present_in": 2
  }
}
```

### processing_metadata.json
Processing statistics

```json
{
  "filename": "data.txt",
  "total_items": 4,
  "items_by_type": {
    "html": 1,
    "json": 2,
    "text": 1
  }
}
```

## ğŸ¯ Usage Scenarios

### Scenario 1: Process Once
```bash
python main.py
# Select option 1
# Done!
```

### Scenario 2: Keep Processing Files
```bash
python main.py watch
# Leave running
# Drop files in inputs/
# They auto-process
```

### Scenario 3: Save to Database
```bash
python main.py db
# Saves to SQLite for queries
```

## ğŸ’¡ Tips

| Tip | Benefit |
|-----|---------|
| Use watch mode | Auto-process new files |
| Use db mode | Query results with SQL |
| Keep file small | Faster processing |
| Mixed formats | Just worksâ„¢ |
| Excel can't handle? | Use dynamic_schema.json |

## â“ FAQ

**Q: Can I process .html files?**  
A: Yes! Any text-based file works.

**Q: Will my data be sent anywhere?**  
A: No! Everything is local only.

**Q: Can I see what it extracts?**  
A: Yes! Check processed_metadata.json

**Q: Can I use the cleaned CSV in Excel?**  
A: Yes! That's exactly what it's for.

**Q: What if my file is large?**  
A: Works fine! Just takes a bit longer.

**Q: Can I modify the extraction logic?**  
A: Yes! Edit etl_pipeline.py (well-commented)

## ğŸ”§ Troubleshooting

| Error | Fix |
|-------|-----|
| `ModuleNotFoundError: pandas` | Run `pip install -r requirement.txt` |
| `No files found` | Put a file in inputs/ folder |
| `Permission denied` | Make sure inputs/ and outputs/ exist |
| Garbled text | File encoding issue (auto-handled) |

## ğŸ“ˆ Performance Tips

- Process files < 10MB for instant results
- Use watch mode to avoid manual runs
- Use database mode for multiple analyses
- Split huge files into smaller chunks

## ğŸ“ Learning Path

1. Read this file first (âœ“ you're here)
2. Run with sample_data.txt
3. Check outputs/cleaned_output.csv
4. Try your own file
5. Read README.md for details
6. Explore etl_pipeline.py code

## âœ¨ Key Features Checklist

- âœ… Detects HTML automatically
- âœ… Parses JSON objects
- âœ… Extracts plain text
- âœ… Creates dynamic schema
- âœ… Normalizes all data
- âœ… Saves as CSV
- âœ… Saves metadata
- âœ… Supports watch mode
- âœ… Optional SQLite
- âœ… Local storage only
- âœ… No APIs
- âœ… Beginner-friendly

## ğŸš€ Getting Started Right Now

```bash
# Copy your file to inputs/
cp your_data.txt inputs/

# Run the pipeline
python main.py

# Press 1 and hit Enter

# Wait 1-5 seconds...

# Open: outputs/cleaned_output.csv
```

Done! Your data is now structured. ğŸ‰

---

**Questions?** Check:
- README.md (detailed)
- SETUP.md (quick setup)
- TEST_DEMO.py (see it in action)
- EXPLANATION.md (how it works)
