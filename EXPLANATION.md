# ğŸ“š ETL Pipeline - Complete Explanation

## What I Built For You

A **simple, beginner-friendly ETL pipeline** that takes messy, unstructured files and turns them into clean, organized data.

**No APIs. No fancy databases. Just pure Python file processing.**

---

## ğŸ—ï¸ Architecture Explained

```
YOUR MIXED FILE (HTML + JSON + TEXT)
           â†“
    [ETLPIPELINE]
           â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
  â†“                  â†“
CLEANED CSV    SCHEMA JSON
```

### What Each Component Does:

| Component | Purpose | Output |
|-----------|---------|--------|
| **read_file()** | Reads your input file (handles UTF-8 and Latin-1) | Raw text content |
| **detect_content_types()** | Finds HTML, JSON, text, base64 patterns | Categorized content |
| **extract_*()** | Pulls out useful data from each format | Structured records |
| **infer_schema()** | Finds all unique fields across all records | Dynamic schema |
| **normalize()** | Makes all records have same columns | Clean DataFrame |
| **load()** | Saves to CSV, JSON, metadata files | Output files |

---

## ğŸ“‚ File Structure

```
ETL-Pipeline/
â”‚
â”œâ”€â”€ main.py                           â† START HERE (Simple CLI)
â”œâ”€â”€ etl_pipeline.py                   â† Core processing engine
â”œâ”€â”€ requirement.txt                   â† Dependencies to install
â”‚
â”œâ”€â”€ inputs/                           â† DROP YOUR FILE HERE
â”‚   â””â”€â”€ your_file.txt
â”‚
â”œâ”€â”€ outputs/                          â† YOUR RESULTS GO HERE
â”‚   â”œâ”€â”€ cleaned_output.csv            â† OPEN THIS IN EXCEL
â”‚   â”œâ”€â”€ dynamic_schema.json           â† Schema info
â”‚   â””â”€â”€ processing_metadata.json      â† Processing stats
â”‚
â””â”€â”€ Documentation
    â”œâ”€â”€ README.md                     â† Full documentation
    â”œâ”€â”€ SETUP.md                      â† Quick setup guide
    â””â”€â”€ TEST_DEMO.py                  â† Shows what happens step-by-step
```

---

## ğŸš€ How to Use (5 Steps)

### Step 1: Install Dependencies
```bash
pip install -r requirement.txt
```

Creates folders:
- `inputs/` - Where you put files
- `outputs/` - Where results go

### Step 2: Put Your File in inputs/
```
inputs/
â””â”€â”€ raw_scraped_data.txt
    (can contain HTML, JSON, text all mixed together)
```

### Step 3: Run the Pipeline
```bash
python main.py
```

You'll see a menu:
```
ğŸš€ SIMPLE ETL PIPELINE

Options:
1. Process existing files in inputs/ folder
2. Watch inputs/ folder for new files (auto-process)
3. Process a specific file
4. View outputs
5. Exit

Enter your choice: 1
```

### Step 4: Press 1 and Enter
The pipeline runs and shows:
```
ğŸš€ Starting ETL Pipeline for: raw_data.txt
==================================================

1ï¸âƒ£ Reading file...
   âœ“ File size: 5432 characters

2ï¸âƒ£ Extracting data...
   âœ“ Extracted 15 items

3ï¸âƒ£ Inferring schema...
   âœ“ Schema with 12 fields

4ï¸âƒ£ Normalizing data...
   âœ“ Created DataFrame: (15, 12)

5ï¸âƒ£ Loading outputs...
   âœ“ Saved cleaned data to: outputs/cleaned_output.csv
   âœ“ Saved schema to: outputs/dynamic_schema.json
   
âœ… ETL Pipeline completed successfully!
```

### Step 5: Check Results
Open `outputs/cleaned_output.csv` in Excel â†’ Your data is structured!

---

## ğŸ“Š What Happens Internally (5 Stages)

### STAGE 1: READ
```
Input: raw_data.txt (5432 bytes)
â†“
Tries UTF-8 encoding
If fails â†’ Tries Latin-1
â†“
Output: Raw text string
```

### STAGE 2: EXTRACT
Finds and extracts:

**HTML Blocks:**
```html
<html>...</html>  â†’  {title, text, links, images}
```

**JSON Objects:**
```json
{"name": "John", "age": 30}  â†’  Flattens nested objects
```

**Plain Text:**
```
"My paragraph here"  â†’  {content, word_count}
```

**Base64 Media:**
```
data:image/png;base64,iVBORw0KG...  â†’  {format, size}
```

### STAGE 3: INFER SCHEMA
Looks at all extracted records and asks:
- What fields exist?
- What data types?
- Which can be NULL?

Creates dynamic schema:
```json
{
  "name": {
    "type": ["str"],
    "nullable": false,
    "present_in": 5
  },
  "age": {
    "type": ["int"],
    "nullable": true,
    "present_in": 3
  }
}
```

### STAGE 4: NORMALIZE
**Before:**
```
Record 1: {name: "John", age: 30}
Record 2: {title: "CEO"}
Record 3: {name: "Jane"}
```

**After:**
```
Record 1: {name: "John", age: 30, title: None}
Record 2: {name: None, age: None, title: "CEO"}
Record 3: {name: "Jane", age: None, title: None}
```

All records have SAME columns âœ“

### STAGE 5: LOAD
Saves 3 files:

**cleaned_output.csv** (for Excel)
```csv
name,age,title
John,30,None
None,None,CEO
Jane,None,None
```

**dynamic_schema.json** (field definitions)
```json
{"name": {...}, "age": {...}, "title": {...}}
```

**processing_metadata.json** (statistics)
```json
{"filename": "data.txt", "items_extracted": 3, ...}
```

---

## ğŸ¯ Real-World Example

### Input File: `sales_data.txt`
```
<html>
  <title>Q4 Sales Report</title>
  <p>Quarterly performance summary</p>
</html>

{"region": "North", "sales": 50000, "target": 45000}
{"region": "South", "sales": 35000, "target": 40000}

Total company revenue increased by 15% this quarter.
Highest performing region was the North region.

{"quarter": "Q4", "year": 2025, "growth": 0.15}
```

### Processing:

```
1. READ: 5.2 KB file read successfully âœ“

2. EXTRACT:
   - HTML: 1 block (title="Q4 Sales Report")
   - JSON: 3 objects (2 regions + 1 summary)
   - Text: 2 paragraphs
   Total: 6 items

3. SCHEMA:
   region, sales, target, quarter, year, growth, title, content, ...

4. NORMALIZE:
   Record 1: region=North, sales=50000, target=45000, title=None, ...
   Record 2: region=South, sales=35000, target=40000, title=None, ...
   Record 3: title=Q4 Sales Report, content=Quarterly..., region=None, ...
   ... and more

5. SAVE:
   âœ“ cleaned_output.csv
   âœ“ dynamic_schema.json
   âœ“ processing_metadata.json
```

### Output: `cleaned_output.csv`
```
region,sales,target,quarter,year,growth,title,content
North,50000,45000,None,None,None,None,None
South,35000,40000,None,None,None,None,None
None,None,None,None,2025,0.15,None,None
None,None,None,Q4,None,None,Q4 Sales Report,Quarterly performance...
... and more
```

**NOW YOU CAN:**
- Open in Excel âœ“
- Import into database âœ“
- Process with pandas âœ“
- Analyze with Python âœ“

---

## ğŸ“ Key Concepts for Beginners

### What is ETL?
- **E**xtract: Get data from different formats
- **T**ransform: Clean and organize it
- **L**oad: Save it for use

### Why This is Useful?
You receive messy files from web scrapers/APIs with:
- Mixed formats (HTML, JSON, text)
- Inconsistent structure
- Duplicate/missing fields

This pipeline turns that into:
- Clean spreadsheet format
- Consistent columns
- Ready for analysis/database

### What's "Dynamic Schema"?
Instead of pre-defining columns, the pipeline:
1. Looks at actual data
2. Finds all possible fields
3. Creates schema automatically
4. Fills missing values with None

**Benefit:** Works with ANY data structure!

---

## ğŸ’¾ Privacy & Storage

âœ… **100% Local Processing**
- All files stay on your machine
- No data sent anywhere
- Full control

âœ… **Local Storage Options**
- File system (default) - inputs/, outputs/
- SQLite database (optional) - outputs/etl_data.db

---

## ğŸ”§ Advanced Features

### Watch Mode
Automatically process new files as they're added:
```bash
python main.py watch
```
Files in `inputs/` â†’ Auto-processed â†’ Results in `outputs/`

### Database Storage
Save results to SQLite:
```bash
python main.py db
```
Creates `outputs/etl_data.db` with tables:
- `processed_data` - All records
- `schemas` - Schema info

### Programmatic Usage
Use in your own Python code:
```python
from etl_pipeline import ETLPipeline

pipeline = ETLPipeline()
df, schema = pipeline.run("myfile.txt")

# Now you have pandas DataFrame
print(df.head())
```

---

## ğŸ“‹ File Formats Supported

**Input Formats:**
- Plain text (.txt)
- HTML (.html)
- JSON (.json)
- Mixed files
- Any text-based format

**Output Formats:**
- CSV (for Excel)
- JSON (schema)
- JSON (metadata)
- SQLite (optional)

---

## âš¡ Performance

| File Size | Processing Time |
|-----------|-----------------|
| < 1 MB | < 1 second |
| 1-10 MB | 1-5 seconds |
| 10-50 MB | 5-30 seconds |
| 50+ MB | 30+ seconds |

---

## ğŸ› Common Issues & Solutions

| Problem | Solution |
|---------|----------|
| "No files in inputs/" | Add a file to inputs/ folder |
| UnicodeDecodeError | Pipeline handles this automatically |
| "JSON parse error" | Normal - invalid JSON is skipped |
| Permission denied | Make sure inputs/ and outputs/ folders exist |

---

## ğŸ“š Documentation Files

| File | Purpose |
|------|---------|
| **README.md** | Complete technical documentation |
| **SETUP.md** | Quick start guide |
| **TEST_DEMO.py** | Shows step-by-step example |
| **etl_pipeline.py** | Core implementation (well-commented) |
| **main.py** | CLI interface |

---

## âœ… What You Get

âœ“ Simple file processing (no APIs)  
âœ“ Auto-detects data types  
âœ“ Handles mixed formats  
âœ“ Creates clean spreadsheets  
âœ“ Local storage only  
âœ“ Optional database  
âœ“ Watch mode for automation  
âœ“ Well-documented code  
âœ“ Beginner-friendly  

---

## ğŸ¯ Next Steps

1. **Install packages:**
   ```bash
   pip install -r requirement.txt
   ```

2. **Test with sample:**
   ```bash
   python main.py
   # Choose option 1
   ```
   Uses `sample_data.txt` if you want to test

3. **Use your own file:**
   Put your file in `inputs/` and process

4. **Check results:**
   Open `outputs/cleaned_output.csv` in Excel

---

## ğŸ‰ Summary

You now have a **complete ETL pipeline** that:
- Takes messy unstructured data
- Extracts useful information
- Organizes it into clean spreadsheets
- Stores everything locally
- Requires zero backend knowledge

Just:
```bash
python main.py
```

And you're done! âœ¨
